---
layout: archive
title: "CV"
permalink: /cv/
author_profile: true

---



## Education

* Ph.D in Information Science, Drexel University, 2026 (expected)
* M.S. in Information Science, University of North Carolina at Chapel Hill, 2021
* B.S. in Management Information System, Dalian University of Technology, 2019

## Work experience

### NLP Research Intern
* **Samsung Research America, Mountain View, California** (September 2025 – now)
  * Working on improving QA Grounding with layout information.

### Applied Scientist Intern
* **Amazon, Seattle, Washington** (June 2025 – September 2025)
  * Designed and implemented a novel framework that generates precise product category definitions through reinforcement learning from verbal feedback, replacing costly manual definition writing
  * Built a modular system leveraging Claude Sonnet 3.5 and Claude Haiku 3.5, including components for the actor, evaluators, self-reflection, and a memory module to store historical reflections
  * Designed a sampling model to extract representative product examples from millions of listings, enabling scalable learning
  * Achieved 97.5% classification accuracy (vs. 94% human-crafted definitions) while generating shorter, clearer outputs
  * Reduced category definition creation from weeks to hours, accelerating business response to new product categories and emerging market demands

### Research Scientist Intern
* **Ping An Technology Research Lab, Palo Alto, California** (March 2025 – June 2025)
  * Designed and optimized both ToB and ToC medical VLM, supporting real-world deployment in general practice settings
  * Fine-tuned 32B, 72B Qwen vision-language large models (VLMs) on 400K multi-turn medical consultation dialogues using LoRA and DeepSpeed on 8×A800 GPUs
  * Developed a RAG (Retrieval-Augmented Generation) pipeline with a knowledge graph of rare diseases and treatments, improving diagnostic accuracy from 82% to 90% while reducing hallucinations
  * Quantized Qwen 72B VLM with GPTQ, decreasing model size by 69.65%
  * Deployed the quantized model using vLLM for inference acceleration, reducing response latency by 50–66% compared to baseline
  
## Skills

* **Model Training**
  * TensorFlow, PyTorch
  * SFT, RLHF, PPO, QLoRA
  * Distributed RPC (Data/Model Parallel)
  * Quantization Aware Training

* **Model Deployment**
  * Docker, Kubernetes, Tensorflow Lite
  * React.js, Django, Node.js, Spring Cloud
  * AWS EC2

* **Programming Languages**
  * Python (transformers, opencv, nltk, sklearn, scipy)
  * Java, JavaScript, C, C#
  * Shell/Scripting
  * SQL/NoSQL

## Service

* Reviewer for the AAAI Conference on Artificial Intelligence (AAAI), 2025
* Reviewer for the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2025
* Reviewer for the 63rd Annual Meetings of the Association for Computing Linguistics (ACL), 2025
* Reviewer for International Conference on Computational Linguistics (COLING), 2024

## Academic Activities

* Oral Presentation at COLING, January 2025
* Oral Presentation at IJCAI XAI workshop, June 2024
* Poster Presentation at AAAI Doctoral Consortium, February 2025
* Poster Presentation at AAAS, February 2024
* Poster Presentation at AAAI IAAI session, Washington D.C., 2023

## Awards & Scholarships

* Phoebe W. Haas Endowed Fellowship for Women Doctoral Students, 2023
* Honorable Mention (2nd place) in Mathematical Contest in Modeling (MCM), USA, 2017
